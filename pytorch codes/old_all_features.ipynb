{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gogxpYH2oS-R"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from skimage import io, color, transform\n","from skimage.feature import local_binary_pattern\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"jsHGvbO7pQI5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Path to the folders containing real and fake images\n","original_folder_path = \"/content/gdrive/MyDrive/dataset/original-cropped-images\"\n","synthetic_folder_path = \"/content/gdrive/MyDrive/dataset/synthetic-cropped-images\""],"metadata":{"id":"0z6g-TjkpUxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"a-4Zt3hApX0D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_and_preprocess_images(folder,half):\n","    images = []\n","\n","    count=0\n","    if half==1:\n","        for filename in os.listdir(folder)[:20000]:\n","            img_path = os.path.join(folder, filename)\n","            img = io.imread(img_path)\n","            img_gray = color.rgb2gray(img)\n","            img_resized = transform.resize(img_gray, (64, 64))  # Resize to a consistent size\n","\n","            # Convert the NumPy array to a PyTorch tensor\n","            img_tensor = torch.tensor(img_resized, dtype=torch.float32).to(device)\n","            images.append(img_tensor)\n","\n","            count+=1\n","            print(count)\n","    if half==2:\n","        for filename in os.listdir(folder)[20000:]:\n","            img_path = os.path.join(folder, filename)\n","            img = io.imread(img_path)\n","            img_gray = color.rgb2gray(img)\n","            img_resized = transform.resize(img_gray, (64, 64))  # Resize to a consistent size\n","\n","            # Convert the NumPy array to a PyTorch tensor\n","            img_tensor = torch.tensor(img_resized, dtype=torch.float32).to(device)\n","            images.append(img_tensor)\n","\n","            count+=1\n","            print(count)\n","\n","    return images"],"metadata":{"id":"XDlHowkZphZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["real_images1 = load_and_preprocess_images(original_folder_path,1)"],"metadata":{"id":"w_0cb_T3poWE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["real_images2 = load_and_preprocess_images(original_folder_path,2)"],"metadata":{"id":"1LkiSVEcs56B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fake_images1 = load_and_preprocess_images(synthetic_folder_path,1)"],"metadata":{"id":"imQPvd5gpp0j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fake_images2 = load_and_preprocess_images(synthetic_folder_path,2)"],"metadata":{"id":"0nmqO99ts9ZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create labels (0 for real, 1 for fake)\n","real_labels_tensor = torch.zeros(len(real_images1)+len(real_images2), dtype=torch.float32).to(device)\n","fake_labels_tensor = torch.ones(len(fake_images1)+len(fake_images2), dtype=torch.float32).to(device)"],"metadata":{"id":"3HYYNS9gpvye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["real_images_tensor1 = torch.stack(real_images1)\n","real_images_tensor2 = torch.stack(real_images2)\n","fake_images_tensor1 = torch.stack(fake_images1)\n","fake_images_tensor2 = torch.stack(fake_images2)"],"metadata":{"id":"r2uMwYDzpxSD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine real and fake data\n","all_images = torch.cat((real_images_tensor1,real_images_tensor2, fake_images_tensor1,fake_images_tensor2), dim=0)\n","all_labels = torch.cat((real_labels_tensor, fake_labels_tensor), dim=0)"],"metadata":{"id":"YHJMuWZTpzSL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_images_numpy = all_images.cpu().numpy()\n","all_labels_numpy = all_labels.cpu().numpy()\n","\n","# Split the dataset into training and testing sets\n","X_train_numpy, X_test_numpy, y_train_numpy, y_test_numpy = train_test_split(\n","    all_images_numpy, all_labels_numpy, test_size=0.2, random_state=42\n",")"],"metadata":{"id":"3a_Tildxp1Sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the NumPy arrays back to PyTorch tensors (and move to GPU if available)\n","X_train = torch.tensor(X_train_numpy, dtype=torch.float32).to(device)\n","X_test = torch.tensor(X_test_numpy, dtype=torch.float32).to(device)\n","train_labels = torch.tensor(y_train_numpy, dtype=torch.float32).to(device)\n","test_labels = torch.tensor(y_test_numpy, dtype=torch.float32).to(device)"],"metadata":{"id":"FOEaDVeSp6mI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LBP"],"metadata":{"id":"ghrJqs0Qqc-o"}},{"cell_type":"code","source":["# Parameters for LBP\n","radius = 1\n","n_points = 8 * radius\n","\n","# Extract LBP features using PyTorch and GPU\n","def extract_lbp_features(images):\n","    features = []\n","    for img in images:\n","        img = img.squeeze(0).cpu().numpy()  # Convert the tensor to NumPy and remove the batch dimension\n","        lbp_img = local_binary_pattern(img, n_points, radius, method='uniform')\n","        hist, _ = np.histogram(lbp_img.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n","        hist = hist.astype(\"float\")\n","        hist /= (hist.sum() + 1e-8)\n","        features.append(hist)\n","    return features"],"metadata":{"id":"2F9yoAUXqeoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lbp_train_features = extract_lbp_features(X_train)\n","lbp_test_features = extract_lbp_features(X_test)\n","lbp_train_features = torch.tensor(lbp_train_features, dtype=torch.float32).to(device)\n","lbp_test_features = torch.tensor(lbp_test_features, dtype=torch.float32).to(device)"],"metadata":{"id":"UBhIU9HqqmUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### HOG"],"metadata":{"id":"eWmTnNr8q4_E"}},{"cell_type":"code","source":["from skimage.feature import hog\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"DzDCjDRUq7CU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to extract HOG features from a list of images\n","def extract_hog_features(images):\n","    hog_features = []\n","    for image in images:\n","        hog_feature = hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2))\n","        hog_features.append(hog_feature)\n","    return np.array(hog_features)"],"metadata":{"id":"qgksSRairKhL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract HOG features from training and testing data\n","hog_train_features = extract_hog_features(X_train_numpy)\n","hog_test_features = extract_hog_features(X_test_numpy)"],"metadata":{"id":"7XZFAMqarUjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standardize the HOG features\n","scaler = StandardScaler()\n","hog_train_features = scaler.fit_transform(hog_train_features)\n","hog_test_features = scaler.transform(hog_test_features)"],"metadata":{"id":"cgd8gECIrkCS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert HOG features to PyTorch tensors\n","hog_train_features = torch.tensor(hog_train_features, dtype=torch.float32).to(device)\n","hog_test_features = torch.tensor(hog_test_features, dtype=torch.float32).to(device)"],"metadata":{"id":"Bt_ZL6IFrxKp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SIFT"],"metadata":{"id":"X-AXtIjht1W-"}},{"cell_type":"code","source":["import cv2"],"metadata":{"id":"FJwcCWCVt3Dm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_sift_features(images):\n","    sift = cv2.SIFT_create()\n","    keypoints_list = []\n","    descriptors_list = []\n","\n","    for image in images:\n","        # Perform SIFT feature extraction on CPU\n","        image8bit = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n","        keypoints, descriptors = sift.detectAndCompute(image8bit, None)\n","\n","        # Move keypoints and descriptors to GPU (if available)\n","        keypoints = [kp.pt for kp in keypoints]\n","\n","        keypoints_list.append(keypoints)\n","        descriptors_list.append(descriptors)\n","\n","    return keypoints_list, descriptors_list"],"metadata":{"id":"2r4Qtya0uAEB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_keypoints_list, train_descriptors_list = extract_sift_features(X_train_numpy)\n","test_keypoints_list, test_descriptors_list = extract_sift_features(X_test_numpy)"],"metadata":{"id":"-NEDwlwHuFaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def keypoints_to_vectors(keypoints_list, descriptors_list):\n","    sift_vector_length = 128  # SIFT descriptor length\n","\n","    features_vectors = []\n","\n","    for descriptors in descriptors_list:\n","        if descriptors is None:\n","            # If no keypoints are detected, add zeros as a placeholder\n","            features_vectors.append(torch.zeros(sift_vector_length).to(device))\n","        else:\n","            # Randomly choose one descriptor from multiple descriptors\n","            random_idx = np.random.randint(0, descriptors.shape[0])\n","            features_vectors.append(torch.tensor(descriptors[random_idx], dtype=torch.float32).to(device))\n","\n","    # Convert the list of vectors to a PyTorch tensor\n","    features_vectors = torch.stack(features_vectors)\n","\n","    # Move the tensor to the GPU (if available)\n","    features_vectors = features_vectors.to(device)\n","\n","    return features_vectors"],"metadata":{"id":"PnXmxdTSuKyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_fv = keypoints_to_vectors(train_keypoints_list, train_descriptors_list)\n","test_fv = keypoints_to_vectors(test_keypoints_list, test_descriptors_list)"],"metadata":{"id":"BpnQ4dwEuREt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the NumPy arrays back to PyTorch tensors (and move to GPU if available)\n","sift_train_features = torch.tensor(train_fv, dtype=torch.float32).to(device)\n","sift_test_features = torch.tensor(test_fv, dtype=torch.float32).to(device)"],"metadata":{"id":"3_PAz0fIuRoY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model"],"metadata":{"id":"r6v4UOUlvQB4"}},{"cell_type":"code","source":["class LbpModel(nn.Module):\n","    def __init__(self, input_size):\n","        super(LbpModel, self).__init__()\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(input_size, 32),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(32),\n","            nn.Dropout(0.5)\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # Flatten the input if not already flattened\n","        x = self.fc1(x)\n","        return x"],"metadata":{"id":"Ytrn2yRRvRwH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class HogModel(nn.Module):\n","    def __init__(self, input_size):\n","        super(HogModel, self).__init__()\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(input_size, 2048),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(2048),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(2048, 1024),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(1024),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc3 = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(512),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc4 = nn.Sequential(\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(256),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc5 = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(128),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc6 = nn.Sequential(\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(64),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc7 = nn.Sequential(\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(32),\n","            nn.Dropout(0.5)\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # Flatten the input if not already flattened\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.fc3(x)\n","        x = self.fc4(x)\n","        x = self.fc5(x)\n","        x = self.fc6(x)\n","        x = self.fc7(x)\n","        return x"],"metadata":{"id":"0rNFacxVQjlI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SiftModel(nn.Module):\n","    def __init__(self, input_size):\n","        super(SiftModel, self).__init__()\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(input_size, 256),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(256),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(128),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc3 = nn.Sequential(\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(64),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc4 = nn.Sequential(\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(32),\n","            nn.Dropout(0.5)\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # Flatten the input if not already flattened\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.fc3(x)\n","        x = self.fc4(x)\n","        return x"],"metadata":{"id":"ZB1ulI4u6abS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lbp_model=LbpModel(lbp_train_features.shape[1])\n","lbp_model.to(device)\n","\n","hog_model=HogModel(hog_train_features.shape[1])\n","hog_model.to(device)\n","\n","sift_model=SiftModel(sift_train_features.shape[1])\n","sift_model.to(device)"],"metadata":{"id":"EI_kUf2Xpbk_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CombinedModel(nn.Module):\n","    def __init__(self,lbp_model,hog_model,sift_model):\n","        super(CombinedModel,self).__init__()\n","        self.lbp_model=lbp_model\n","        self.hog_model=hog_model\n","        self.sift_model=sift_model\n","        self.fc1=nn.Sequential(\n","            nn.Linear(96, 48),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(48),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc2=nn.Sequential(\n","            nn.Linear(48, 24),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(24),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc3=nn.Sequential(\n","            nn.Linear(24, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self,lbp_features,hog_features,sift_features):\n","        lbp_output=self.lbp_model(lbp_features)\n","        hog_output=self.hog_model(hog_features)\n","        sift_output=self.sift_model(sift_features)\n","\n","        combined_output=torch.cat((lbp_output,hog_output,sift_output),dim=1)\n","\n","        x=self.fc1(combined_output)\n","        x=self.fc2(x)\n","        x=self.fc3(x)\n","\n","        return x"],"metadata":{"id":"d5NCsADiqgSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_model=CombinedModel(lbp_model,hog_model,sift_model)\n","combined_model.to(device)"],"metadata":{"id":"M8wR0y-psnja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the loss function and optimizer\n","criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n","optimizer = optim.Adam(combined_model.parameters(), lr=0.01)  # Adam optimizer with a learning rate of 0.001"],"metadata":{"id":"-CBNouOxtCc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the number of training epochs\n","num_epochs=250\n","batch_size=32"],"metadata":{"id":"FVHyUgnGtKPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lbp_train_dataset=TensorDataset(lbp_train_features,train_labels)\n","lbp_train_loader=DataLoader(lbp_train_dataset,batch_size,shuffle=True)\n","lbp_test_dataset=TensorDataset(lbp_test_features,test_labels)\n","lbp_test_loader=DataLoader(lbp_test_dataset,batch_size,shuffle=False)"],"metadata":{"id":"yHy6KyRUtpsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hog_train_dataset=TensorDataset(hog_train_features,train_labels)\n","hog_train_loader=DataLoader(hog_train_dataset,batch_size,shuffle=True)\n","hog_test_dataset=TensorDataset(hog_test_features,test_labels)\n","hog_test_loader=DataLoader(hog_test_dataset,batch_size,shuffle=False)"],"metadata":{"id":"qbuyP0qTuyBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sift_train_dataset=TensorDataset(sift_train_features,train_labels)\n","sift_train_loader=DataLoader(sift_train_dataset,batch_size,shuffle=True)\n","sift_test_dataset=TensorDataset(sift_test_features,test_labels)\n","sift_test_loader=DataLoader(sift_test_dataset,batch_size,shuffle=False)"],"metadata":{"id":"nwUeRRo3uyow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training\n","for epoch in range(num_epochs):\n","    combined_model.train()\n","    total_loss=0.0\n","\n","    for (lbp_batch_data,lbp_labels),(hog_batch_data,hog_labels),(sift_batch_data,sift_labels) in zip(lbp_train_loader,hog_train_loader,sift_train_loader):\n","        optimizer.zero_grad()\n","        lbp_batch_data,hog_batch_data,sift_batch_data=lbp_batch_data.to(device),hog_batch_data.to(device),sift_batch_data.to(device)\n","\n","        output=combined_model(lbp_batch_data,hog_batch_data,sift_batch_data)\n","        loss=criterion(output,hog_labels.view(-1,1).float().to(device))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss+=loss.item()\n","\n","    avg_train_loss = total_loss / len(lbp_train_loader)\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {avg_train_loss:.4f}\")"],"metadata":{"id":"cSiu_dIgvGXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# testing\n","combined_model.eval()\n","\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for (lbp_batch_data,lbp_labels),(hog_batch_data,hog_labels),(sift_batch_data,sift_labels) in zip(lbp_test_loader,hog_test_loader,sift_test_loader):\n","        lbp_batch_data,hog_batch_data,sift_batch_data=lbp_batch_data.to(device),hog_batch_data.to(device),sift_batch_data.to(device)\n","        output=combined_model(lbp_batch_data,hog_batch_data,sift_batch_data)\n","\n","        predicted = (output>=0.5).float()\n","        total+=hog_labels.size(0)\n","\n","        for i in range(hog_labels.size(0)):\n","            if predicted[i][0]==hog_labels[i]:\n","                correct += 1\n","\n","accuracy=correct/total\n","print(f\"Test Accuracy: {accuracy:.4f}%\")"],"metadata":{"id":"nmYhRuSNxW6E"},"execution_count":null,"outputs":[]}]}