{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIgJh98lD1bM","outputId":"aff1a4ec-cbec-4afe-a975-dc9e21048bbf","executionInfo":{"status":"ok","timestamp":1696997861964,"user_tz":-330,"elapsed":4530,"user":{"displayName":"Mohammed Anas Danish","userId":"15304837075160102803"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.2)\n"]}],"source":["!pip install dlib"]},{"cell_type":"code","source":["import os\n","import dlib\n","import cv2\n","import math\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, ReLU, MaxPooling2D, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"PYHyQSa7EV5x","executionInfo":{"status":"ok","timestamp":1696997869846,"user_tz":-330,"elapsed":5616,"user":{"displayName":"Mohammed Anas Danish","userId":"15304837075160102803"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-mghTVlEc39","outputId":"5223e98f-baa7-46d0-a18a-927da258da83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Path to the folders containing real and fake images\n","real_folder_path = \"/content/gdrive/MyDrive/celebdf/original-cropped-images\"\n","fake_folder_path = \"/content/gdrive/MyDrive/celebdf/synthetic-cropped-images\""],"metadata":{"id":"B-PPUmXoEqYB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load and preprocess images\n","def load_and_preprocess_images(folder, max_images=6500):\n","    images = []\n","    count = 0\n","    for filename in os.listdir(folder):\n","        if count >= max_images:\n","            break\n","        print(count)\n","        if filename.endswith(\".jpg\"):\n","            img_path = os.path.join(folder, filename)\n","            # img = dlib.load_rgb_image(img_path)\n","            img = cv2.imread(img_path)\n","            img = cv2.resize(img, (64,64))\n","            images.append(np.array(img))\n","            count += 1\n","    return images"],"metadata":{"id":"mbVEBdAnEx_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the face pose estimator from DLIB\n","# face_detector = dlib.shape_predictor(\"/content/gdrive/MyDrive/shape_predictor_68_face_landmarks.dat\")\n","\n","face_detector = dlib.get_frontal_face_detector()\n","# pose_predictor = dlib.shape_predictor(\"~/Documents/PES1UG20CS646_HitheshDN/capstone/summer/code/shape_predictor_68_face_landmarks.dat\")\n","predictor = dlib.shape_predictor(\"/content/gdrive/MyDrive/shape_predictor_68_face_landmarks.dat\")"],"metadata":{"id":"JTHrHnwqEzPl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load and preprocess real and fake images\n","real_images = np.array(load_and_preprocess_images(real_folder_path))\n"],"metadata":{"id":"LlWYR6T-GETU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fake_images = np.array(load_and_preprocess_images(fake_folder_path))"],"metadata":{"id":"9pTWfaNcYlaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#not needed\n","# fake_images = fake_images[:9000]"],"metadata":{"id":"5v3uw2yIYlkv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(Flatten()(real_images[0]))\n","# print(real_images[0].flatten())\n","# print(real_images)\n","# print(fake_images)\n"],"metadata":{"id":"CpAJhmDuJ5HV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# image_height, image_width, num_channels = real_images[0].shape\n","# print(real_images.shape)\n","# print(fake_images.shape)\n","\n","# print(real_images[0].shape)\n","# print(real_images[1].shape)\n","# print(fake_images[0].shape)\n","# print(fake_images[1].shape)\n","\n"],"metadata":{"id":"spDfmia6K7t0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine real and fake images and create labels\n","all_images = np.concatenate((real_images, fake_images), axis=0)\n","labels = np.concatenate((np.ones(len(real_images)), np.zeros(len(fake_images))), axis=0)"],"metadata":{"id":"L4Y0znt7LFvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data into training and validation sets\n","train_images, val_images, train_labels, val_labels = train_test_split(all_images, labels, test_size=0.3, random_state=42)\n","\n","# print(train_images)\n","# print(val_images)\n","# print(train_labels)\n","# print(val_labels)"],"metadata":{"id":"eYVeHtuuLLRz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# def extract_face_pose_features(images):\n","#     features = []\n","#     for img in images:\n","#         face_landmarks = face_pose_estimator(img, dlib.rectangle(0, 0, img.shape[1], img.shape[0]))\n","#         pose_feature = np.array([(point.x, point.y) for point in face_landmarks.parts()])\n","#         print(pose_feature)\n","#         features.append(pose_feature)\n","#     return features\n","\n","def find_rotation_matrix(landmarks):\n","    # Convert landmarks to numpy array for easier indexing\n","    landmarks_array = np.array([[p.x, p.y] for p in landmarks.parts()])\n","\n","    # Calculate the center of the eyes\n","    left_eye_center = landmarks_array[36:42].mean(axis=0)\n","    right_eye_center = landmarks_array[42:48].mean(axis=0)\n","\n","    # Calculate the angle of rotation (yaw) using the eye centers\n","    dy = right_eye_center[1] - left_eye_center[1]\n","    dx = right_eye_center[0] - left_eye_center[0]\n","    angle = math.atan2(dy, dx) * 180.0 / math.pi\n","\n","    #calculate the rotation matrix\n","    rotation_matrix = cv2.getRotationMatrix2D(tuple(left_eye_center), angle, 1.0)\n","    rotation_matrix = np.vstack((rotation_matrix, np.array([0, 0, 1])))\n","    return rotation_matrix\n","\n","def find_translation_vector(image,landmarks):\n","    landmarks_array = np.array([[p.x, p.y] for p in landmarks.parts()])\n","\n","    # to find center of face\n","    face_center = landmarks_array.mean(axis=0)\n","\n","    # Calculate the translation vector as the displacement from the center\n","    translation_vector = face_center - np.array([image.shape[1] / 2, image.shape[0] / 2])\n","\n","    return translation_vector\n","\n","\n","def face_pose_estimator(image):\n","    features = []\n","    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    faces = face_detector(gray_image)\n","    if len(faces) > 0:\n","        face = faces[0]\n","        landmarks = predictor(gray_image, face)\n","\n","        # Calculate yaw, pitch, roll angles using rotation matrix\n","        # You can modify the code from the previous response to extract these angles\n","        # rotation_matrix = dlib.get_face_chips(image, [landmarks])[0].pose_rot_matrix\n","        rotation_matrix = find_rotation_matrix(landmarks)\n","        # translation_vector = dlib.get_face_chip(image, landmarks).pose_T\n","        translation_vector = find_translation_vector(image,landmarks)\n","        # print(rotation_matrix)\n","        # print(translation_vector)\n","\n","        yaw = math.degrees(math.atan2(rotation_matrix[1, 0], rotation_matrix[0, 0]))\n","        pitch = math.degrees(math.atan2(-rotation_matrix[2, 0], math.sqrt(rotation_matrix[2, 1]**2 + rotation_matrix[2, 2]**2)))\n","        roll = math.degrees(math.atan2(rotation_matrix[2, 1], rotation_matrix[2, 2]))\n","\n","        # Append angles to the features list\n","        features=[yaw, pitch, roll]\n","    else:\n","        # Append some default values if no face detected\n","        features=[0, 0, 0]\n","    return features"],"metadata":{"id":"rnLYXbzaQwih","executionInfo":{"status":"ok","timestamp":1696997885227,"user_tz":-330,"elapsed":557,"user":{"displayName":"Mohammed Anas Danish","userId":"15304837075160102803"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["final_training_set = []\n","final_validating_set = []\n","for image in train_images:\n","  img_feature = face_pose_estimator(image)\n","  temp = []\n","  temp.extend(image.flatten())\n","  temp.extend(img_feature)\n","  final_training_set.append(temp)\n","for image in val_images:\n","  img_feature = face_pose_estimator(image)\n","  temp = []\n","  temp.extend(image.flatten())\n","  temp.extend(img_feature)\n","  final_validating_set.append(temp)\n","\n","final_training_set = np.array(final_training_set)\n","final_validating_set = np.array(final_validating_set)\n","\n","# print(face_pose_estimator(train_images[0]))\n","# print(\"----------------------------------\")\n","# print(face_pose_estimator(train_images[1]))\n","# print(\"----------------------------------\")\n","\n","# print(face_pose_estimator(train_images[2]))\n","# print(\"----------------------------------\")\n","\n","# print(face_pose_estimator(fake_images[1]))"],"metadata":{"id":"Os5FDYAyKLj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define your neural network architecture\n","def build_model(input_shape=(12291,)):\n","    input_layer = Input(shape=input_shape)\n","    x = Dense(512, activation='relu')(input_layer)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(256, activation='relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(128, activation='relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(64, activation='relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(32, activation='relu')(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.5)(x)\n","    output_layer = Dense(1, activation='sigmoid')(x)  # Binary classification\n","\n","    model = Model(inputs=input_layer, outputs=output_layer)\n","    return model"],"metadata":{"id":"TBw9f9tGLN_P","executionInfo":{"status":"ok","timestamp":1696997901855,"user_tz":-330,"elapsed":2,"user":{"displayName":"Mohammed Anas Danish","userId":"15304837075160102803"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Define model architecture\n","# feature_dim = train_features_flatten.shape[1]\n","model = build_model()"],"metadata":{"id":"QwpnMwU3THQL","executionInfo":{"status":"ok","timestamp":1696997908123,"user_tz":-330,"elapsed":2747,"user":{"displayName":"Mohammed Anas Danish","userId":"15304837075160102803"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"nTmx_C7GLRpW","executionInfo":{"status":"ok","timestamp":1696997910600,"user_tz":-330,"elapsed":5,"user":{"displayName":"Mohammed Anas Danish","userId":"15304837075160102803"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","epochs = 10\n","\n","model.fit(final_training_set, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(final_validating_set, val_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fg2CK7C-kK8C","outputId":"7b94d16a-9092-4b56-e50b-a3e72472525c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","223/223 [==============================] - 9s 17ms/step - loss: 0.8382 - accuracy: 0.5523 - val_loss: 0.6427 - val_accuracy: 0.6395\n","Epoch 2/10\n","223/223 [==============================] - 3s 13ms/step - loss: 0.7158 - accuracy: 0.6005 - val_loss: 0.6374 - val_accuracy: 0.6461\n","Epoch 3/10\n","223/223 [==============================] - 3s 11ms/step - loss: 0.6744 - accuracy: 0.6200 - val_loss: 0.6290 - val_accuracy: 0.6451\n","Epoch 4/10\n","223/223 [==============================] - 3s 11ms/step - loss: 0.6485 - accuracy: 0.6426 - val_loss: 0.6167 - val_accuracy: 0.6553\n","Epoch 5/10\n","223/223 [==============================] - 2s 11ms/step - loss: 0.6310 - accuracy: 0.6591 - val_loss: 0.5980 - val_accuracy: 0.6773\n","Epoch 6/10\n","223/223 [==============================] - 4s 16ms/step - loss: 0.6180 - accuracy: 0.6744 - val_loss: 0.5820 - val_accuracy: 0.7193\n","Epoch 7/10\n","223/223 [==============================] - 3s 14ms/step - loss: 0.5979 - accuracy: 0.6928 - val_loss: 0.5691 - val_accuracy: 0.7272\n","Epoch 8/10\n","223/223 [==============================] - 3s 16ms/step - loss: 0.5826 - accuracy: 0.7027 - val_loss: 0.5702 - val_accuracy: 0.7255\n","Epoch 9/10\n","223/223 [==============================] - 3s 13ms/step - loss: 0.5773 - accuracy: 0.7163 - val_loss: 0.5491 - val_accuracy: 0.7308\n","Epoch 10/10\n","223/223 [==============================] - 3s 15ms/step - loss: 0.5630 - accuracy: 0.7260 - val_loss: 0.5416 - val_accuracy: 0.7515\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7b5821675ea0>"]},"metadata":{},"execution_count":95}]},{"cell_type":"code","source":["batch_size = 64\n","epochs = 10\n","\n","model.fit(final_training_set, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(final_validating_set, val_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvi7Mj-ekLHr","outputId":"df5fe06c-0fd9-4aa5-9db7-2ad10e3d3750"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","112/112 [==============================] - 2s 17ms/step - loss: 0.5759 - accuracy: 0.7146 - val_loss: 0.5962 - val_accuracy: 0.7111\n","Epoch 2/10\n","112/112 [==============================] - 2s 14ms/step - loss: 0.5659 - accuracy: 0.7203 - val_loss: 0.5954 - val_accuracy: 0.7058\n","Epoch 3/10\n","112/112 [==============================] - 2s 15ms/step - loss: 0.5601 - accuracy: 0.7224 - val_loss: 0.5653 - val_accuracy: 0.7209\n","Epoch 4/10\n","112/112 [==============================] - 2s 19ms/step - loss: 0.5588 - accuracy: 0.7303 - val_loss: 0.5427 - val_accuracy: 0.7469\n","Epoch 5/10\n","112/112 [==============================] - 2s 17ms/step - loss: 0.5588 - accuracy: 0.7294 - val_loss: 0.5847 - val_accuracy: 0.7255\n","Epoch 6/10\n","112/112 [==============================] - 2s 13ms/step - loss: 0.5566 - accuracy: 0.7372 - val_loss: 0.6108 - val_accuracy: 0.6999\n","Epoch 7/10\n","112/112 [==============================] - 2s 14ms/step - loss: 0.5585 - accuracy: 0.7310 - val_loss: 0.5546 - val_accuracy: 0.7423\n","Epoch 8/10\n","112/112 [==============================] - 2s 14ms/step - loss: 0.5482 - accuracy: 0.7367 - val_loss: 0.5230 - val_accuracy: 0.7699\n","Epoch 9/10\n","112/112 [==============================] - 2s 14ms/step - loss: 0.5345 - accuracy: 0.7533 - val_loss: 0.6004 - val_accuracy: 0.6533\n","Epoch 10/10\n","112/112 [==============================] - 2s 14ms/step - loss: 0.5391 - accuracy: 0.7488 - val_loss: 0.5355 - val_accuracy: 0.7482\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7b57f420add0>"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["# Train the model\n","batch_size = 32\n","epochs = 10\n","\n","model.fit(final_training_set, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(final_validating_set, val_labels))"],"metadata":{"id":"pq7W0XFvMWjy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"694d9f09-c6b8-477c-a220-a3ea7d7f3558"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","254/254 [==============================] - 10s 13ms/step - loss: 0.9021 - accuracy: 0.5654 - val_loss: 0.6511 - val_accuracy: 0.6445\n","Epoch 2/10\n","254/254 [==============================] - 3s 12ms/step - loss: 0.7286 - accuracy: 0.6080 - val_loss: 0.6245 - val_accuracy: 0.6534\n","Epoch 3/10\n","254/254 [==============================] - 3s 12ms/step - loss: 0.6797 - accuracy: 0.6250 - val_loss: 0.6111 - val_accuracy: 0.6588\n","Epoch 4/10\n","254/254 [==============================] - 4s 14ms/step - loss: 0.6429 - accuracy: 0.6495 - val_loss: 0.5935 - val_accuracy: 0.6849\n","Epoch 5/10\n","254/254 [==============================] - 3s 13ms/step - loss: 0.6106 - accuracy: 0.6863 - val_loss: 0.6336 - val_accuracy: 0.6573\n","Epoch 6/10\n","254/254 [==============================] - 3s 12ms/step - loss: 0.5939 - accuracy: 0.6990 - val_loss: 0.5667 - val_accuracy: 0.7277\n","Epoch 7/10\n","254/254 [==============================] - 3s 11ms/step - loss: 0.5802 - accuracy: 0.7103 - val_loss: 0.5558 - val_accuracy: 0.7410\n","Epoch 8/10\n","254/254 [==============================] - 3s 13ms/step - loss: 0.5683 - accuracy: 0.7185 - val_loss: 0.5686 - val_accuracy: 0.7351\n","Epoch 9/10\n","254/254 [==============================] - 4s 15ms/step - loss: 0.5631 - accuracy: 0.7258 - val_loss: 0.6657 - val_accuracy: 0.5953\n","Epoch 10/10\n","254/254 [==============================] - 3s 11ms/step - loss: 0.5555 - accuracy: 0.7334 - val_loss: 0.5692 - val_accuracy: 0.7223\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7b58216be560>"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","source":["# Evaluate the model\n","test_loss, test_accuracy = model.evaluate(final_validating_set, val_labels)\n","print(\"Test accuracy:\", test_accuracy)"],"metadata":{"id":"uQrWE6OnhB5I","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dbd97e62-f3ce-473b-a7ba-f320f5f13741"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["96/96 [==============================] - 0s 4ms/step - loss: 0.5355 - accuracy: 0.7482\n","Test accuracy: 0.7481943368911743\n"]}]},{"cell_type":"code","source":["%pip install torchviz\n","import torch\n","import torchviz\n","\n","# pass your input to the model (in your case image of what ever size the input layer takes)\n","dot = torchviz.make_dot(model(torch.randn(1, 32, 32)))\n","\n","dot.render(\"model_architecture.png\", format=\"png\")"],"metadata":{"id":"htD5DeaGGIEQ","colab":{"base_uri":"https://localhost:8080/","height":671},"executionInfo":{"status":"error","timestamp":1696998172706,"user_tz":-330,"elapsed":4482,"user":{"displayName":"Mohammed Anas Danish","userId":"15304837075160102803"}},"outputId":"adbfbc74-2f19-4bc5-fed3-66c6e44ffb0c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.1+cu118)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (17.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-029af9164269>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# pass your input to the model (in your case image of what ever size the input layer takes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_architecture.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\u001b[0m in \u001b[0;36mdisplay_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'as_list'"]}]}]}